---
sidebar_label: 'LLM 개관'
sidebar_position: 2
---


# Large Language Models(LLM) 개관

언어 모델이란? 가장 자연스러운 단어 시퀀스를 찾아내는 모델
가장 자연스러운 단어 시퀀스를 찾아내려면 이전 단어들이 주어졌을 때 다음 단어를 예측하도록 해야 합니다.

예를 들어, “나는 이 자료를 만들기 위해 오늘도 밤을” 뒤에 올 서술어는 무엇일까요?

- [ ] 지새웠다.
- [ ] 까서 구워 먹었다.

문맥상 ‘1번 지새웠다’가 맞지만 만일 먹는 밤을 의미한다면 뒤에 올 서술어가 달라지겠죠?


이렇게 이전 단계에서의 데이터 정보를 기억해서 다음 단계의 처리에 사용하는 것을 시퀀스 분석이라고 합니다.

그렇다면 짧은 단어가 아닌 하나의 문장으로 다른 문장을 예측해야 한다면 어떨까요?

데이터가 길어지면 그만큼 긴 시퀀스에서 정보를 유지해야 하므로 비효율적일 수밖에 없습니다.


이러한 의존성을 문제를 해결하기 위해 Seq2Seq, LSTM, GRU 등 여러 시퀀스 분석 모델들이 차례로 등장했습니다, 그 중 최종적으로 Transformer 모델이 등장하고 이 모든 문제를 해결할 수 있었습니다.

