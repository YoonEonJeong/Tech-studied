---
sidebar_label: 'Data Warehouse'
sidebar_position: 2
---

---
# 데이터 웨어하우스 사전 지식

해당 자료는 다음을 참고하여 제작하였습니다
[Burak Dogu](https://medium.com/@burak_doguu/building-a-data-warehouse-pipeline-with-kafka-cassandra-airflow-and-snowflake-fa2a8906b043), [AWS](https://aws.amazon.com/ko/compare/the-difference-between-kafka-and-spark/)

## 개요
데이터 웨어하우스를 구축하기 위해 알아야 할 사전 지식을 정리하였습니다

## DW 파이프라인 구축을 위한 사전 지식

### Kafka
#### 개념

**Apache Kafka**는 고성능, 고가용성을 제공하는 분산 스트리밍 플랫폼입니다. **Kafka**는 실시간으로 스트리밍 데이터를 수집하고 처리하는 데 최적화된 분산 데이터 스토어입니다, 대규모 메시지 처리에 최적화되어 있습니다.

 스트리밍 데이터는 수천 개의 데이터 원본에서 연속적으로 생성되는 데이터로, 보통 데이터 레코드를 동시에 전송합니다. 스트리밍 플랫폼은 이러한 지속적인 데이터 유입을 처리하고 데이터를 순차적이고 점진적으로 처리해야 합니다.

#### 용도

**Kafka**는 데이터 스트림에 적응하는 실시간 스트리밍 데이터 파이프라인과 애플리케이션을 구축하는 데 주로 사용됩니다. 메시징, 스토리지, 스트림 처리를 결합해 과거 및 실시간 데이터 모두의 저장 및 분석을 허용합니다.

#### 작동 방식

**Kafka**는 대기열과 게시-구독이라는 두 가지 메시징 모델을 결합하여 소비자에게 각 모델의 주요 이점을 제공합니다. 대기열을 사용하면 데이터 처리를 여러 소비자 인스턴스에 분산할 수 있어 확장성이 뛰어납니다. 

게시-구독 접근 방식은 다중 구독자이지만 모든 메시지가 모든 구독자에게 전송되므로 여러 작업자 프로세스에 작업을 분산하는 데 사용할 수 없습니다.

**Kafka**는 파티셔닝된 로그 모델을 사용하여 이러한 두 개의 솔루션을 통합합니다. 로그는 순서가 지정된 레코드 시퀀스이며 이러한 로그는 여러 구독자에 해당하는 세그먼트 또는 파티션으로 나뉩니다. 즉, 동일한 주제에 여러 구독자가 있을 수 있고 각 구독자에게 파티션을 할당하여 확장성을 높일 수 있습니다.

마지막으로 **Kafka**의 모델은 재생 기능을 제공하므로 데이터 스트림을 읽는 여러 독립 애플리케이션이 자체 속도에 따라 독립적으로 작동할 수 있습니다.

<!-- ![kafka](fig3.png) -->

#### 장점

**확장성**: **Kafka**의 파티셔닝된 로그 모델을 사용하면 데이터를 여러 서버에 분산할 수 있으므로 단일 서버에 담을 수 있는 수준 이상으로 데이터를 확장할 수 있습니다. 

**신속함**: **Kafka**는 데이터 스트림을 분리하므로 지연 시간이 매우 짧아 속도가 극도로 빠릅니다. 

**내구성**: 파티션은 여러 서버에 분산되어 복제되며 데이터는 모두 디스크에 기록됩니다. 이렇게 하면 서버 장애로부터 데이터를 보호할 수 있어 데이터의 내결함성과 내구성을 높일 수 있습니다. 


#### 데이터 웨어하우스와의 연결

**Kafka**는 다양한 소스로부터 실시간 데이터를 수집하여 데이터 웨어하우스로 전송하는 데 사용될 수 있습니다. 이는 실시간 데이터 분석과 보고를 가능하게 합니다.


### Spark

#### 개념

**Spark**는 빅 데이터 워크로드에 사용되는 오픈 소스 분산 처리 시스템입니다. **Apache Spark**는 인 메모리 캐시 및 최적화된 쿼리 실행을 활용하여 모든 크기의 데이터에 대해 빠른 분석 쿼리를 실행합니다.또한 일괄 처리, 대화형 쿼리, 실시간 분석, 기계 학습, 그래프 처리 등 여러 워크로드에서 코드 재사용을 지원합니다.

#### 용도
**Kafk**병렬 분산 알고리즘을 사용하여 빅 데이터 세트를 처리를 필요로 할 때 사용합니다

#### 작동 방식
**Spark**는 인 메모리 처리를 수행하고, 작업의 단계 수를 줄이고, 여러 병렬 작업에서 데이터를 재사용하여 **MapReduce**의 한계를 해결하기 위해 만들어졌습니다.

**Spark**를 사용하면 데이터를 메모리로 읽어 들이고, 작업을 수행하고, 결과를 다시 쓰는 과정에서 단 한 단계만 필요하므로 실행 속도가 훨씬 빨라집니다. 또한 Spark는 인 메모리 캐시를 사용하여 데이터를 재사용하여 동일한 데이터 세트에서 함수를 반복적으로 호출하는 기계 학습 알고리즘의 속도를 크게 높입니다.

데이터 재사용은 메모리에 캐시되고 여러 **Spark** 작업에서 재사용되는 객체 모음인 Resilient Distributed Dataset(RDD)에 대한 추상화인 `DataFrames` 생성을 통해 수행됩니다. 이는 특히 기계 학습 및 대화형 분석을 수행할 때 **Spark**가 **MapReduce**보다 몇 배 더 빨라지도록 지연 시간을 크게 줄여줍니다

#### 장점
**신속함**: 인 메모리 캐시 및 최적화된 쿼리 실행을 통해 모든 크기의 데이터에 대해 빠른 분석 쿼리를 실행할 수 있습니다.

**개발자 친화적**: 애플리케이션을 구축할 수 있도록 다양한 언어를 제공합니다. 이러한 API를 사용하면 필요한 코드 양을 크게 줄이는 간단한 고급 연산자 뒤에 분산 처리의 복잡성이 숨겨지기 때문에 개발자가 작업을 쉽게 수행할 수 있습니다.

**다중 워크로드**: 대화형 쿼리, 실시간 분석, 기계 학습, 그래프 처리 등 여러 워크로드를 실행할 수 있는 기능을 제공합니다. 하나의 애플리케이션으로 여러 워크로드를 원활하게 결합할 수 있습니다.

### Kafk vs Spark

**Kafka**

스트림 처리 엔진입니다. 

스트림 처리에서는 작은 단위의 데이터를 실시간 흐름으로 연속적으로 처리합니다.

Spark는 Spark 스트리밍 모듈을 기본 분산 아키텍처에 추가 기능으로 추가했습니다만, Kafka에서 대부분의 스트리밍 데이터 사용 사례에서 더 낮은 지연 시간과 더 높은 처리량을 제공할 수 있습니다

**Kafka Workflow**

실시간 분산 처리 엔진을 사용하여 데이터 생산자와 소비자를 연결합니다.

핵심 구성 요소는 다음과 같습니다.

- 소비자와 생산자 간의 거래를 용이하게 하는 브로커
- 서로 다른 서버에 있는 여러 브로커로 구성된 클러스터

생산자는 **Kafka** 클러스터에 정보를 게시하고, 소비자는 처리를 위해 정보를 검색합니다. 각 **Kafka** 브로커는 주제에 따라 메시지를 구성한 다음 브로커를 여러 파티션으로 나눕니다. 특정 주제에 공통 관심을 가진 여러 소비자가 관련 파티션을 구독하여 데이터 스트리밍을 시작할 수 있습니다. 

**Spark**

분산 데이터 처리 엔진입니다.

배치 처리 엔진은 단일 워크로드에서 매우 많은 양의 데이터를 처리합니다.

**Spark Workflow**

Spark Core는 기본 **Spark** 기능을 포함하는 주요 구성 요소입니다. 이 기능에는 분산 데이터 처리, 메모리 관리, 작업 스케줄링 및 디스패치, 스토리지 시스템과의 상호 작용이 포함됩니다. 

**Spark**는 데이터 전환 및 일괄 처리 워크플로를 지원하는 여러 순차 계층이 있는 분산형 기본-보조 아키텍처를 사용합니다. 프라이머리 노드는 데이터 처리 작업을 예약하고 워커 노드에 할당하는 중앙 코디네이터입니다. 

데이터 처리 쿼리 발생 시 다음 단계가 수행됩니다.
- 프라이머리 노드는 변경할 수 없는 데이터 복사본을 여러 개 생성합니다.
- 그래프 스케줄러를 사용하여 요청을 일련의 처리 작업으로 나눕니다.
- 작업을 Spark Core로 전달하고, Spark Core는 작업을 예약하고 특정 워커 노드에 할당합니다.
- 최종적으로 워커 노드가 작업을 완료하면 클러스터 관리자를 통해 결과를 프라이머리 노드에 반환합니다

#### 유사점
- 빅 데이터 처리: 여러 서버에 분산된 데이터 파이프라인을 제공하여 대량의 데이터를 실시간으로 수집하고 처리합니다. 다양한 소스 간에 효율적이고 지속적인 데이터 전달이 필요한 빅 데이터 사용 사례를 지원합니다.

- 데이터 다양성: 비정형, 반정형 및 정형 데이터를 수집할 수 있습니다. 이를 통해 다형 데이터 파이프라인을 생성할 수 있습니다. 일반적으로 사용되는 일반 텍스트, JSON, XM,L SQL 및 기타 데이터 형식을 모두 지원합니다.

- 확장성: 확장성이 뛰어난 데이터 스트리밍 엔진이며 수직 및 수평으로 모두 확장할 수 있습니다.

#### 차이점
| 기능         | Kafka                                          | Spark                                         |
|--------------|------------------------------------------------|-----------------------------------------------|
| ETL          | ETL 함수를 위한 Kafka Connect API 및 Kafka Streams API가 필요합니다. | 기본적으로 ETL을 지원합니다.                       |
| 지연 시간     | 매우 짧은 지연 시간. 각 수신 이벤트에 대해 실제 시간을 제공합니다. | 짧은 지연 시간. RAM에서 읽기/쓰기 작업을 수행합니다. |
| 프로그래밍 언어 | 데이터 변환 함수를 구현하려면 추가 라이브러리가 필요합니다.  | 데이터 변환 및 기계 학습 작업을 위해 Java, Python, Scala 및 R을 지원합니다. |
| 가용성        | 다른 서버에 데이터 파티션을 백업합니다. 활성 파티션에 장애가 발생할 경우 요청을 백업에 전달합니다. | 여러 노드에서 영구 데이터를 유지합니다. 노드 장애 시 결과를 다시 계산합니다. |
| 데이터 소스   | 여러 데이터 원본을 동시에 지원할 수 있습니다.      | 단일 데이터 원본에 연결합니다. 여러 데이터 소스로 스트리밍하려면 Spark Structured Streaming이 필요합니다. |
| 처리 모델     | 지속적인 처리를 가능하게 하는 분산 스트리밍 플랫폼입니다. 실시간 데이터 수신에 초점을 맞춥니다. | 실시간 데이터 흐름을 캡처하고, 데이터를 작은 배치로 전환하여 처리합니다. Kafka의 실시간 처리 속도에 필적할 수 없습니다. |
| 사용 시기       | 실시간 데이터 스트리밍 및 메시징에 최적화. 다양한 애플리케이션 또는 서비스 간에 메시지를 신속하게 교환할 수 있도록 설계됨. | 대량의 데이터 처리 및 분석, 기계 학습에 적합. 일괄 처리 및 실시간 데이터 처리를 위한 고급 분석 기능 제공. |
| 데이터 스토리지 | 메시지를 영구 로그 파일에 저장. 정전이나 장애가 발생해도 데이터 보호. 다른 서버에 데이터 백업합니다. | RAM을 주 저장소로 사용하여 빠른 데이터 처리. 필요시 디스크 사용. HDFS, 클라우드 스토리지와 호환됩니다. |
| API            | 다양한 프로그래밍 언어를 지원하는 광범위한 API 제공합니다. 데이터 스트림 게시, 구독 및 처리 가능. | 실시간 데이터 스트림 변환에 중점을 둔 API. Java, Python, Scala 지원. 데이터 변환 및 처리에 특화되었습니다. |

### Cassandra

#### 개념

**Apache Cassandra**는 고가용성과 확장성을 제공하는 분산 NoSQL 데이터베이스 시스템입니다. 대량의 데이터를 처리할 수 있으며, 여러 데이터 센터에 걸쳐 데이터를 분산시킬 수 있습니다.

#### 용도

**Cassandra**는 대규모 데이터의 빠른 쓰기와 읽기 작업에 적합합니다. 사용자 데이터 관리, IoT 데이터 저장, 실시간 분석 데이터베이스 등에 쓰입니다.

#### 데이터 웨어하우스와의 연결

**Cassandra**는 데이터 웨어하우스에서 대용량의 실시간 데이터를 처리하고 저장하는 데 사용될 수 있으며, 특히 저지연 요구 사항이 있는 대량의 데이터를 처리하는 데 뛰어납니다. Kafka와 Cassandra 간의 통합은 실시간 데이터 저장을 가능하게 하며, 이 데이터는 나중에 분석 및 보고를 위해 사용될 수 있습니다. 실시간 데이터 저장이 가능하므로, 이벤트 데이터나 시계열 데이터에 적합합니다 

### Airflow

#### 개념

**Apache Airflow**는 워크플로우 관리 플랫폼으로, 데이터 파이프라인의 스케줄링과 모니터링을 자동화하는 데 사용됩니다. 파이썬으로 작성된 DAG(방향 비순환 그래프)를 통해 작업 흐름을 정의합니다.

#### 용도

**Airflow**는 복잡한 데이터 파이프라인의 종속성을 관리하고, 데이터 처리 작업의 실행을 자동화합니다. 데이터 추출, 변환, 로딩(ETL) 작업에 주로 사용됩니다.

####  데이터 웨어하우스와의 연결

**Airflow**는 데이터 웨어하우스로 데이터를 추출, 변환 및 로드하는 프로세스를 자동화하는 데 중요한 역할을 합니다. 이는 데이터의 정확성과 최신성을 보장합니다.

### Snowflake

#### 개념

**Snowflake**는 클라우드 기반의 데이터 웨어하우스 서비스로, 데이터의 저장, 처리, 분석 기능을 제공합니다. 유연한 스케일링과 별도의 저장 및 컴퓨팅 레이어를 특징으로 합니다. Snowpark를 사용하여 Python과 같은 프로그래밍 언어로 코드를 작성하고 이를 Snowflake 데이터 클라우드 내에서 직접 실행할 수 있습니다.

#### 용도

**Snowflake**는 다양한 데이터 소스로부터 데이터를 통합하고, 대규모 데이터 세트에 대한 복잡한 쿼리를 실행할 수 있습니다. 데이터 분석, 보고, 데이터 공유 등에 사용됩니다.

####  데이터 웨어하우스 구축에 결합

**Snowflake**는 최종적인 데이터 웨어하우스 솔루션으로, Kafka로부터 실시간 데이터를 수신하고, Cassandra에서 대용량 데이터를 처리하며, Airflow를 통해 데이터 파이프라인을 관리하고 자동화합니다. 이렇게 하여 효율적이고 강력한 데이터 웨어하우스 환경을 구축할 수 있습니다.


### DBT

#### 개념
데이터 분석가와 데이터 엔지니어가 데이터 변환 작업을 보다 효율적으로 수행할 수 있도록 설계된 오픈 소스 커맨드 라인 도구입니다. SQL을 사용하여 데이터 웨어하우스 내에서 데이터 변환 로직을 정의하고 실행할 수 있게 해주며, 데이터 모델링, 테스트, 문서화 등의 과정을 자동화합니다.

또한 **dbt**는 **dbt** Labs에 의해 유지 관리되는 데이터 엔지니어링 프레임워크로, Snowflake와 같은 클라우드 데이터 플랫폼을 활용하는 현대 데이터 아키텍처에 유용합니다. 오픈소스 프로젝트인 cosmos를 통해 dbt Core 프로젝트를 Apache Airflow DAG 및 Task Group으로 실행할 수 있습니다.

#### 용도
- ETL/ELT 파이프라인 구축: 데이터 추출 및 로딩(EL) 후 변환(T) 단계에서 **dbt**를 사용하여 데이터를 정제, 집계 및 변환할 수 있습니다.
- 데이터 분석 및 보고: 분석가는 **dbt**를 사용하여 데이터 모델을 정의하고, 이를 기반으로 보고서와 대시보드를 생성할 수 있습니다.
- 데이터 거버넌스 및 품질 관리: 데이터의 품질을 보장하고 문서화를 통해 데이터 거버넌스 정책을 지원할 수 있습니다.

#### 작동 방식
- 개발: 사용자는 SQL 파일로 데이터 변환 로직을 작성합니다. 이 파일들은 **dbt**에서 "모델"로 간주됩니다.
- 빌드: `dbt run` 명령어를 실행하면, **dbt**는 모델, 테스트, 문서화 작업을 순차적으로 수행합니다.
- 테스트: `dbt test` 명령어를 사용하여 데이터 모델에 대한 데이터 품질 검사를 자동화합니다.
- 문서화: `dbt docs generate` 명령어로 프로젝트의 문서화를 생성하고, `docs serve` 명령어로 웹 서버에서 문서를 호스팅할 수 있습니다.

#### 사용 사례
- ETL/ELT 파이프라인 구축: 데이터 추출 및 로딩(EL) 후 변환(T) 단계에서 **dbt**를 사용하여 데이터를 정제, 집계 및 변환할 수 있습니다.
- 데이터 분석 및 보고: 분석가는 **dbt**를 사용하여 데이터 모델을 정의하고, 이를 기반으로 보고서와 대시보드를 생성할 수 있습니다.
- 데이터 거버넌스 및 품질 관리: 데이터의 품질을 보장하고 문서화를 통해 데이터 거버넌스 정책을 지원할 수 있습니다.
